{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "Project_CS5542_DataPrep.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eP22e6VSExP"
      },
      "source": [
        "# Project : Classifying and Searching News Articles from Key Phrases\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp2eIWcHGBzb"
      },
      "source": [
        "### Data Preperation Part (Mehmet Acikgoz)\n",
        "\n",
        "1.   Data Sampling\n",
        "2.   Keywords Extraction\n",
        "3.   Creating the Final Output \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtImJycYYe3d"
      },
      "source": [
        "**Part 1 - Data Sampling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOmTsRRk8A1t"
      },
      "source": [
        "We are going to work with the news data from kaggle (https://www.kaggle.com/rmisra/news-category-dataset). The data has more than 200.000 records. Since it requires a lot of computing power to  work with all the dataset. We have decided to take a sample of data which has around 20.000 records.\n",
        "The following part reads the original dataset and spits out  equal number of records from each category in the news to a new file. The news in each category is randomly selected.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "415fiaQ1GX9a"
      },
      "source": [
        "We need to mount the google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeNi6-yV_10V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "394c73c0-31e7-4c00-97d5-ea456c63b335"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QI2T5TZU_GN"
      },
      "source": [
        "We need to have the rake library for the keyword extraction. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdsfND7f_cdR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9480d3fc-ea9a-46b9-b3bb-a4b03256c647"
      },
      "source": [
        "!pip install rake-nltk"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rake-nltk\n",
            "  Downloading https://files.pythonhosted.org/packages/8e/c4/b4ff57e541ac5624ad4b20b89c2bafd4e98f29fd83139f3a81858bdb3815/rake_nltk-1.0.4.tar.gz\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from rake-nltk) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->rake-nltk) (1.15.0)\n",
            "Building wheels for collected packages: rake-nltk\n",
            "  Building wheel for rake-nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rake-nltk: filename=rake_nltk-1.0.4-py2.py3-none-any.whl size=7819 sha256=d21459da0e964e6aef7b9145d78011bc57a719eb5a969f92398da089ae5fc2c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/ef/92/fc/271b3709e71a96ffe934b27818946b795ac6b9b8ff8682483f\n",
            "Successfully built rake-nltk\n",
            "Installing collected packages: rake-nltk\n",
            "Successfully installed rake-nltk-1.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaJYwaegIBvJ"
      },
      "source": [
        "Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUWQ6vEP_cdV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "307b17ff-f979-4c0b-d913-834e3e39510e"
      },
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import Request, urlopen\n",
        "\n",
        "from rake_nltk import Metric, Rake"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X58oNBjVIIrV"
      },
      "source": [
        "Defining global variables "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RM_f52li_cdZ"
      },
      "source": [
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "punctuation_list = list(punctuation)\n",
        "punctuation_list.extend(['‘','’','”','―',',”','“','\"','{','}'])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tL7dzv6rIN3q"
      },
      "source": [
        "Reading the original dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUPdOAmn7_6W"
      },
      "source": [
        "input_file_name = \"News_Category_Dataset_v2.json\"\n",
        "import json\n",
        "original_file_name = \"/content/drive/My Drive/BigDataAnalyticsAndApplications/Project/dataset/\"+input_file_name\n",
        "original_data = []\n",
        "with open(original_file_name) as f:\n",
        "    for line in f:\n",
        "        original_data.append(json.loads(line))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSeArijCIk3F"
      },
      "source": [
        "Converting the list of data into pandas dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oTcfroI8AFE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "945433df-1885-4872-ce20-a999c2f049c2"
      },
      "source": [
        "original_df = pd.json_normalize(original_data)\n",
        "original_df.head(3)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>headline</th>\n",
              "      <th>authors</th>\n",
              "      <th>link</th>\n",
              "      <th>short_description</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CRIME</td>\n",
              "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
              "      <td>Melissa Jeltsen</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
              "      <td>She left her husband. He killed their children...</td>\n",
              "      <td>2018-05-26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ENTERTAINMENT</td>\n",
              "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
              "      <td>Andy McDonald</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
              "      <td>Of course it has a song.</td>\n",
              "      <td>2018-05-26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ENTERTAINMENT</td>\n",
              "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
              "      <td>Ron Dicker</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
              "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
              "      <td>2018-05-26</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        category  ...        date\n",
              "0          CRIME  ...  2018-05-26\n",
              "1  ENTERTAINMENT  ...  2018-05-26\n",
              "2  ENTERTAINMENT  ...  2018-05-26\n",
              "\n",
              "[3 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmCyCjOcIrR4"
      },
      "source": [
        "Checking on the number of unique categories in the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXldA1zl8ATl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ca413f0-d024-498d-bffe-6350008eb8fc"
      },
      "source": [
        "original_df['category'].value_counts()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "POLITICS          32739\n",
              "WELLNESS          17827\n",
              "ENTERTAINMENT     16058\n",
              "TRAVEL             9887\n",
              "STYLE & BEAUTY     9649\n",
              "PARENTING          8677\n",
              "HEALTHY LIVING     6694\n",
              "QUEER VOICES       6314\n",
              "FOOD & DRINK       6226\n",
              "BUSINESS           5937\n",
              "COMEDY             5175\n",
              "SPORTS             4884\n",
              "BLACK VOICES       4528\n",
              "HOME & LIVING      4195\n",
              "PARENTS            3955\n",
              "THE WORLDPOST      3664\n",
              "WEDDINGS           3651\n",
              "WOMEN              3490\n",
              "IMPACT             3459\n",
              "DIVORCE            3426\n",
              "CRIME              3405\n",
              "MEDIA              2815\n",
              "WEIRD NEWS         2670\n",
              "GREEN              2622\n",
              "WORLDPOST          2579\n",
              "RELIGION           2556\n",
              "STYLE              2254\n",
              "SCIENCE            2178\n",
              "WORLD NEWS         2177\n",
              "TASTE              2096\n",
              "TECH               2082\n",
              "MONEY              1707\n",
              "ARTS               1509\n",
              "FIFTY              1401\n",
              "GOOD NEWS          1398\n",
              "ARTS & CULTURE     1339\n",
              "ENVIRONMENT        1323\n",
              "COLLEGE            1144\n",
              "LATINO VOICES      1129\n",
              "CULTURE & ARTS     1030\n",
              "EDUCATION          1004\n",
              "Name: category, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuimmROrHfMP"
      },
      "source": [
        "Selecting predefined number of records from each category and saving it back to a new file with the same input format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQpCXe-h8AQK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "137050ae-d16b-4a0e-87d8-6a75e6163b35"
      },
      "source": [
        "num_of_records_for_each_category = 500\n",
        "data_df = pd.DataFrame(columns=['category', 'headline', 'authors', 'link', 'short_description', 'date'])\n",
        "for col in original_df['category'].unique():\n",
        "  print(\"data sampled in the category \", col)\n",
        "  indices = original_df[original_df['category']==col].index\n",
        "  random_items = np.random.choice(indices, num_of_records_for_each_category, False)\n",
        "  df_temp = original_df.iloc[random_items]\n",
        "  data_df = pd.concat([data_df, df_temp], ignore_index=True, sort=False)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data sampled in the category  CRIME\n",
            "data sampled in the category  ENTERTAINMENT\n",
            "data sampled in the category  WORLD NEWS\n",
            "data sampled in the category  IMPACT\n",
            "data sampled in the category  POLITICS\n",
            "data sampled in the category  WEIRD NEWS\n",
            "data sampled in the category  BLACK VOICES\n",
            "data sampled in the category  WOMEN\n",
            "data sampled in the category  COMEDY\n",
            "data sampled in the category  QUEER VOICES\n",
            "data sampled in the category  SPORTS\n",
            "data sampled in the category  BUSINESS\n",
            "data sampled in the category  TRAVEL\n",
            "data sampled in the category  MEDIA\n",
            "data sampled in the category  TECH\n",
            "data sampled in the category  RELIGION\n",
            "data sampled in the category  SCIENCE\n",
            "data sampled in the category  LATINO VOICES\n",
            "data sampled in the category  EDUCATION\n",
            "data sampled in the category  COLLEGE\n",
            "data sampled in the category  PARENTS\n",
            "data sampled in the category  ARTS & CULTURE\n",
            "data sampled in the category  STYLE\n",
            "data sampled in the category  GREEN\n",
            "data sampled in the category  TASTE\n",
            "data sampled in the category  HEALTHY LIVING\n",
            "data sampled in the category  THE WORLDPOST\n",
            "data sampled in the category  GOOD NEWS\n",
            "data sampled in the category  WORLDPOST\n",
            "data sampled in the category  FIFTY\n",
            "data sampled in the category  ARTS\n",
            "data sampled in the category  WELLNESS\n",
            "data sampled in the category  PARENTING\n",
            "data sampled in the category  HOME & LIVING\n",
            "data sampled in the category  STYLE & BEAUTY\n",
            "data sampled in the category  DIVORCE\n",
            "data sampled in the category  WEDDINGS\n",
            "data sampled in the category  FOOD & DRINK\n",
            "data sampled in the category  MONEY\n",
            "data sampled in the category  ENVIRONMENT\n",
            "data sampled in the category  CULTURE & ARTS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVEhUh7EJpau",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "493b89a2-3ba0-42fa-d1fc-b6d0f04aeed0"
      },
      "source": [
        "data_df.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>headline</th>\n",
              "      <th>authors</th>\n",
              "      <th>link</th>\n",
              "      <th>short_description</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CRIME</td>\n",
              "      <td>Leonel Contreras Guilty: California Teen Convi...</td>\n",
              "      <td></td>\n",
              "      <td>https://www.huffingtonpost.com/entry/leonel-co...</td>\n",
              "      <td>Leonel Contreras was tried as an adult and wil...</td>\n",
              "      <td>2012-11-04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CRIME</td>\n",
              "      <td>Driver Sandwiched By 2 Semi-Trucks Describes H...</td>\n",
              "      <td>Steven Hoffer</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/kaleb-whi...</td>\n",
              "      <td></td>\n",
              "      <td>2015-01-19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CRIME</td>\n",
              "      <td>Person Of Interest Detained After California M...</td>\n",
              "      <td></td>\n",
              "      <td>https://www.huffingtonpost.com/entry/person-of...</td>\n",
              "      <td>Riverside County sheriff's deputies called the...</td>\n",
              "      <td>2015-12-12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CRIME</td>\n",
              "      <td>A Famous Hacker On What It Is Like Being Pursu...</td>\n",
              "      <td>Quora, ContributorThe best answer to any question</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/a-famous-...</td>\n",
              "      <td>I surrendered to the U.S. Marshals Service aft...</td>\n",
              "      <td>2015-05-19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CRIME</td>\n",
              "      <td>Ohio State University Attack Leaves 11 Injured</td>\n",
              "      <td>Willa Frej and Ryan Grenoble</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/ohio-stat...</td>\n",
              "      <td>Police fatally shot the suspect, OSU student A...</td>\n",
              "      <td>2016-11-28</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  category  ...        date\n",
              "0    CRIME  ...  2012-11-04\n",
              "1    CRIME  ...  2015-01-19\n",
              "2    CRIME  ...  2015-12-12\n",
              "3    CRIME  ...  2015-05-19\n",
              "4    CRIME  ...  2016-11-28\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osB7KfEn1sTV"
      },
      "source": [
        "**Part 2 - Keywords Extraction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGfZri_2AJtN"
      },
      "source": [
        "In this part, we are going to work on the sample dataset. \n",
        "\n",
        "Firstly,  we will go over each record in the dataset and access the orignal news article, then we will save the main text by scraping the news. We are going to use beatifulsoup to scrape the articles.\n",
        "\n",
        "Secondly, after scraping the text of each article, we will implement keyword extraction by the rake library. Then we will have a new column 'key_words'  for each article. This new column will be used in the modelling later.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUwFtoIe8ADj"
      },
      "source": [
        "# converting the dataframe into a list of dictionary\n",
        "data = data_df.to_dict(orient='records')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkQC8m9jRlg1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db7daecb-aaa7-49e3-e3aa-4e4e9d8c9cfc"
      },
      "source": [
        "len(data)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20500"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKoSV-1Z_cdi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a449e114-02fb-41a9-b7e9-58f4888640e5"
      },
      "source": [
        "FROM_RECORDS = 0\n",
        "TO_RECORDS = 400\n",
        "NUM_RECORDS = TO_RECORDS-FROM_RECORDS\n",
        "NUM_RECORDS"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEVKzg-VNc4Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88ce62fd-44c7-499d-de2d-1c43d019696b"
      },
      "source": [
        "data = data[FROM_RECORDS:TO_RECORDS]\n",
        "print('Number of rows in the data', len(data))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of rows in the data 400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wBRGyAPkmy1"
      },
      "source": [
        "def read_article_content(site):\n",
        "    '''\n",
        "    Reads all the article text and returns as a string\n",
        "    '''\n",
        "    hdr = {'User-Agent': 'Mozilla/5.0'}\n",
        "    req = Request(site,headers=hdr)\n",
        "    page = urlopen(req)\n",
        "    soup = BeautifulSoup(page)\n",
        "#     soup = BeautifulSoup(page, \"lxml\")\n",
        "\n",
        "    content = [soup.title.text]\n",
        "    for div in soup.findAll(\"div\", {\"class\": \"content-list-component yr-content-list-text text\"}):\n",
        "        sentence = '\\n' + div.text.strip()\n",
        "        sentence = sentence.encode(\"ascii\", \"ignore\").decode()  # To ignore the non-ascii characters in the article.\n",
        "        sentence = sentence.replace(\"\\\"\",\"\").replace(\"{\",\"\").replace(\"}\",\"\").replace(\"...\",\"\") # To clean some of the punctuation characters\n",
        "        content.append(sentence)\n",
        "\n",
        "    article = ' '.join([str(elem) for elem in content])  \n",
        "    \n",
        "    return article\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOCReD0r_cdz"
      },
      "source": [
        "def get_me_keywords(text):\n",
        "  '''\n",
        "  Returns upto predifned number of the keywords by using the RAKE algorithm given the text\n",
        "\n",
        "  '''    \n",
        "  NUM_PHRASES = 20; # number of phrases in the top ranking\n",
        "    \n",
        "  rake = Rake(stopwords= stop_words, punctuations= punctuation_list)\n",
        "  text_array = text.split('\\n')\n",
        "  rake.extract_keywords_from_sentences(text_array)\n",
        "    \n",
        "    \n",
        "  my_phrases = rake.get_ranked_phrases()\n",
        "\n",
        "  if len(my_phrases) < NUM_PHRASES:\n",
        "    NUM_PHRASES = len(my_phrases)\n",
        "        \n",
        "  return my_phrases[:NUM_PHRASES]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLMM3wyj3hVS"
      },
      "source": [
        "def validateurl(url):\n",
        "  '''\n",
        "  There are some systematic typos in the hyperlinks. \n",
        "  This method Validates and returns the correct form of the the hyperlink given in the dataset. \n",
        "  '''\n",
        "\n",
        "  if url.startswith('https://www.huffingtonpost.comhttp://'):\n",
        "    url = url.replace('https://www.huffingtonpost.com','')\n",
        "  return url"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdKFMraD_cd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a479fd5-589d-483b-c88c-65d729dcf4b7"
      },
      "source": [
        "my_list = []\n",
        "start_index = 0\n",
        "interval = 100\n",
        "for i in range(NUM_RECORDS):\n",
        "    try:\n",
        "      url = data[i]['link']\n",
        "      url = validateurl(url)\n",
        "      article = read_article_content(url)\n",
        "      keywords = get_me_keywords(article)\n",
        "      data[i][\"key_words\"] = keywords\n",
        "\n",
        "      if (i > 0) and ((i % interval) == (interval-1)) :\n",
        "        data_in_box = data[start_index:start_index+interval]\n",
        "        df_in_box = pd.json_normalize(data_in_box)\n",
        "        out_filename = str(FROM_RECORDS+start_index)+\"_\"+str(FROM_RECORDS+i)+\".json\"\n",
        "        df_in_box.to_csv (r'/content/drive/My Drive/BigDataAnalyticsAndApplications/Project/dataset/' + out_filename, index = False, header=True)\n",
        "        print(i, 'is done')\n",
        "        start_index = i+1\n",
        "\n",
        "\n",
        "    except:\n",
        "      print(\"Something went wrong at \", (FROM_RECORDS+i) )\n",
        "      data[i][\"key_words\"] = []\n",
        "      my_list.append((FROM_RECORDS+i))\n",
        "      continue\n",
        "\n",
        "print(\"Completed\")\n",
        "\n",
        "\n",
        "with open(r'/content/drive/My Drive/BigDataAnalyticsAndApplications/Project/dataset/errors_file.txt', 'a+') as myfile:\n",
        "    for item in my_list:\n",
        "        myfile.write(\"%s\\n\" % item)\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99 is done\n",
            "Something went wrong at  189\n",
            "199 is done\n",
            "299 is done\n",
            "399 is done\n",
            "Completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLSeo4Q0yLsM"
      },
      "source": [
        "**Part 3 - Combining all the files into a single one for use. (Final Part)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmpX16IitvWg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b810b52-7b5b-4bab-8e0a-fc4ddc358c21"
      },
      "source": [
        "data_final = pd.DataFrame(columns=['category', 'headline', 'authors', 'link', 'short_description', 'date','key_words'])\n",
        "for i in range(0,20500,100):\n",
        "  start=i \n",
        "  end = i+99\n",
        "  file_name = str(start)+\"_\"+str(end)+\".json\"\n",
        "  full_path = '/content/drive/My Drive/BigDataAnalyticsAndApplications/Project/dataset/'+file_name\n",
        "\n",
        "\n",
        "  try:\n",
        "    df = pd.read_csv(full_path)\n",
        "    data_final = pd.concat([data_final, df], ignore_index=True, sort=False)    \n",
        "  except:\n",
        "    print(\"Something went wrong at the file\",file_name)\n",
        "    continue"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Something went wrong at the file 400_499.json\n",
            "Something went wrong at the file 500_599.json\n",
            "Something went wrong at the file 600_699.json\n",
            "Something went wrong at the file 700_799.json\n",
            "Something went wrong at the file 800_899.json\n",
            "Something went wrong at the file 900_999.json\n",
            "Something went wrong at the file 1000_1099.json\n",
            "Something went wrong at the file 1100_1199.json\n",
            "Something went wrong at the file 1200_1299.json\n",
            "Something went wrong at the file 1300_1399.json\n",
            "Something went wrong at the file 1400_1499.json\n",
            "Something went wrong at the file 1500_1599.json\n",
            "Something went wrong at the file 1600_1699.json\n",
            "Something went wrong at the file 1700_1799.json\n",
            "Something went wrong at the file 1800_1899.json\n",
            "Something went wrong at the file 1900_1999.json\n",
            "Something went wrong at the file 2000_2099.json\n",
            "Something went wrong at the file 2100_2199.json\n",
            "Something went wrong at the file 2200_2299.json\n",
            "Something went wrong at the file 2300_2399.json\n",
            "Something went wrong at the file 2400_2499.json\n",
            "Something went wrong at the file 2500_2599.json\n",
            "Something went wrong at the file 2600_2699.json\n",
            "Something went wrong at the file 2700_2799.json\n",
            "Something went wrong at the file 2800_2899.json\n",
            "Something went wrong at the file 2900_2999.json\n",
            "Something went wrong at the file 3000_3099.json\n",
            "Something went wrong at the file 3100_3199.json\n",
            "Something went wrong at the file 3200_3299.json\n",
            "Something went wrong at the file 3300_3399.json\n",
            "Something went wrong at the file 3400_3499.json\n",
            "Something went wrong at the file 3500_3599.json\n",
            "Something went wrong at the file 3600_3699.json\n",
            "Something went wrong at the file 3700_3799.json\n",
            "Something went wrong at the file 3800_3899.json\n",
            "Something went wrong at the file 3900_3999.json\n",
            "Something went wrong at the file 4000_4099.json\n",
            "Something went wrong at the file 4100_4199.json\n",
            "Something went wrong at the file 4200_4299.json\n",
            "Something went wrong at the file 4300_4399.json\n",
            "Something went wrong at the file 4400_4499.json\n",
            "Something went wrong at the file 4500_4599.json\n",
            "Something went wrong at the file 4600_4699.json\n",
            "Something went wrong at the file 4700_4799.json\n",
            "Something went wrong at the file 4800_4899.json\n",
            "Something went wrong at the file 4900_4999.json\n",
            "Something went wrong at the file 5000_5099.json\n",
            "Something went wrong at the file 5100_5199.json\n",
            "Something went wrong at the file 5200_5299.json\n",
            "Something went wrong at the file 5300_5399.json\n",
            "Something went wrong at the file 5400_5499.json\n",
            "Something went wrong at the file 5500_5599.json\n",
            "Something went wrong at the file 5600_5699.json\n",
            "Something went wrong at the file 5700_5799.json\n",
            "Something went wrong at the file 5800_5899.json\n",
            "Something went wrong at the file 5900_5999.json\n",
            "Something went wrong at the file 6000_6099.json\n",
            "Something went wrong at the file 6100_6199.json\n",
            "Something went wrong at the file 6200_6299.json\n",
            "Something went wrong at the file 6300_6399.json\n",
            "Something went wrong at the file 6400_6499.json\n",
            "Something went wrong at the file 6500_6599.json\n",
            "Something went wrong at the file 6600_6699.json\n",
            "Something went wrong at the file 6700_6799.json\n",
            "Something went wrong at the file 6800_6899.json\n",
            "Something went wrong at the file 6900_6999.json\n",
            "Something went wrong at the file 7000_7099.json\n",
            "Something went wrong at the file 7100_7199.json\n",
            "Something went wrong at the file 7200_7299.json\n",
            "Something went wrong at the file 7300_7399.json\n",
            "Something went wrong at the file 7400_7499.json\n",
            "Something went wrong at the file 7500_7599.json\n",
            "Something went wrong at the file 7600_7699.json\n",
            "Something went wrong at the file 7700_7799.json\n",
            "Something went wrong at the file 7800_7899.json\n",
            "Something went wrong at the file 7900_7999.json\n",
            "Something went wrong at the file 8000_8099.json\n",
            "Something went wrong at the file 8100_8199.json\n",
            "Something went wrong at the file 8200_8299.json\n",
            "Something went wrong at the file 8300_8399.json\n",
            "Something went wrong at the file 8400_8499.json\n",
            "Something went wrong at the file 8500_8599.json\n",
            "Something went wrong at the file 8600_8699.json\n",
            "Something went wrong at the file 8700_8799.json\n",
            "Something went wrong at the file 8800_8899.json\n",
            "Something went wrong at the file 8900_8999.json\n",
            "Something went wrong at the file 9000_9099.json\n",
            "Something went wrong at the file 9100_9199.json\n",
            "Something went wrong at the file 9200_9299.json\n",
            "Something went wrong at the file 9300_9399.json\n",
            "Something went wrong at the file 9400_9499.json\n",
            "Something went wrong at the file 9500_9599.json\n",
            "Something went wrong at the file 9600_9699.json\n",
            "Something went wrong at the file 9700_9799.json\n",
            "Something went wrong at the file 9800_9899.json\n",
            "Something went wrong at the file 9900_9999.json\n",
            "Something went wrong at the file 10000_10099.json\n",
            "Something went wrong at the file 10100_10199.json\n",
            "Something went wrong at the file 10200_10299.json\n",
            "Something went wrong at the file 10300_10399.json\n",
            "Something went wrong at the file 10400_10499.json\n",
            "Something went wrong at the file 10500_10599.json\n",
            "Something went wrong at the file 10600_10699.json\n",
            "Something went wrong at the file 10700_10799.json\n",
            "Something went wrong at the file 10800_10899.json\n",
            "Something went wrong at the file 10900_10999.json\n",
            "Something went wrong at the file 11000_11099.json\n",
            "Something went wrong at the file 11100_11199.json\n",
            "Something went wrong at the file 11200_11299.json\n",
            "Something went wrong at the file 11300_11399.json\n",
            "Something went wrong at the file 11400_11499.json\n",
            "Something went wrong at the file 11500_11599.json\n",
            "Something went wrong at the file 11600_11699.json\n",
            "Something went wrong at the file 11700_11799.json\n",
            "Something went wrong at the file 11800_11899.json\n",
            "Something went wrong at the file 11900_11999.json\n",
            "Something went wrong at the file 12000_12099.json\n",
            "Something went wrong at the file 12100_12199.json\n",
            "Something went wrong at the file 12200_12299.json\n",
            "Something went wrong at the file 12300_12399.json\n",
            "Something went wrong at the file 12400_12499.json\n",
            "Something went wrong at the file 12500_12599.json\n",
            "Something went wrong at the file 12600_12699.json\n",
            "Something went wrong at the file 12700_12799.json\n",
            "Something went wrong at the file 12800_12899.json\n",
            "Something went wrong at the file 12900_12999.json\n",
            "Something went wrong at the file 13000_13099.json\n",
            "Something went wrong at the file 13100_13199.json\n",
            "Something went wrong at the file 13200_13299.json\n",
            "Something went wrong at the file 13300_13399.json\n",
            "Something went wrong at the file 13400_13499.json\n",
            "Something went wrong at the file 13500_13599.json\n",
            "Something went wrong at the file 13600_13699.json\n",
            "Something went wrong at the file 13700_13799.json\n",
            "Something went wrong at the file 13800_13899.json\n",
            "Something went wrong at the file 13900_13999.json\n",
            "Something went wrong at the file 14000_14099.json\n",
            "Something went wrong at the file 14100_14199.json\n",
            "Something went wrong at the file 14200_14299.json\n",
            "Something went wrong at the file 14300_14399.json\n",
            "Something went wrong at the file 14400_14499.json\n",
            "Something went wrong at the file 14500_14599.json\n",
            "Something went wrong at the file 14600_14699.json\n",
            "Something went wrong at the file 14700_14799.json\n",
            "Something went wrong at the file 14800_14899.json\n",
            "Something went wrong at the file 14900_14999.json\n",
            "Something went wrong at the file 15000_15099.json\n",
            "Something went wrong at the file 15100_15199.json\n",
            "Something went wrong at the file 15200_15299.json\n",
            "Something went wrong at the file 15300_15399.json\n",
            "Something went wrong at the file 15400_15499.json\n",
            "Something went wrong at the file 15500_15599.json\n",
            "Something went wrong at the file 15600_15699.json\n",
            "Something went wrong at the file 15700_15799.json\n",
            "Something went wrong at the file 15800_15899.json\n",
            "Something went wrong at the file 15900_15999.json\n",
            "Something went wrong at the file 16000_16099.json\n",
            "Something went wrong at the file 16100_16199.json\n",
            "Something went wrong at the file 16200_16299.json\n",
            "Something went wrong at the file 16300_16399.json\n",
            "Something went wrong at the file 16400_16499.json\n",
            "Something went wrong at the file 16500_16599.json\n",
            "Something went wrong at the file 16600_16699.json\n",
            "Something went wrong at the file 16700_16799.json\n",
            "Something went wrong at the file 16800_16899.json\n",
            "Something went wrong at the file 16900_16999.json\n",
            "Something went wrong at the file 17000_17099.json\n",
            "Something went wrong at the file 17100_17199.json\n",
            "Something went wrong at the file 17200_17299.json\n",
            "Something went wrong at the file 17300_17399.json\n",
            "Something went wrong at the file 17400_17499.json\n",
            "Something went wrong at the file 17500_17599.json\n",
            "Something went wrong at the file 17600_17699.json\n",
            "Something went wrong at the file 17700_17799.json\n",
            "Something went wrong at the file 17800_17899.json\n",
            "Something went wrong at the file 17900_17999.json\n",
            "Something went wrong at the file 18000_18099.json\n",
            "Something went wrong at the file 18100_18199.json\n",
            "Something went wrong at the file 18200_18299.json\n",
            "Something went wrong at the file 18300_18399.json\n",
            "Something went wrong at the file 18400_18499.json\n",
            "Something went wrong at the file 18500_18599.json\n",
            "Something went wrong at the file 18600_18699.json\n",
            "Something went wrong at the file 18700_18799.json\n",
            "Something went wrong at the file 18800_18899.json\n",
            "Something went wrong at the file 18900_18999.json\n",
            "Something went wrong at the file 19000_19099.json\n",
            "Something went wrong at the file 19100_19199.json\n",
            "Something went wrong at the file 19200_19299.json\n",
            "Something went wrong at the file 19300_19399.json\n",
            "Something went wrong at the file 19400_19499.json\n",
            "Something went wrong at the file 19500_19599.json\n",
            "Something went wrong at the file 19600_19699.json\n",
            "Something went wrong at the file 19700_19799.json\n",
            "Something went wrong at the file 19800_19899.json\n",
            "Something went wrong at the file 19900_19999.json\n",
            "Something went wrong at the file 20000_20099.json\n",
            "Something went wrong at the file 20100_20199.json\n",
            "Something went wrong at the file 20200_20299.json\n",
            "Something went wrong at the file 20300_20399.json\n",
            "Something went wrong at the file 20400_20499.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nj0otRJ_yjDc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d239fbc-0253-440b-eb5a-2dda647b0182"
      },
      "source": [
        "data_final.shape"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7kGW9tUyl6G"
      },
      "source": [
        "# To delete any empty keywords in the sampled dataset.\n",
        "data_final.drop(data_final[data_final['key_words'] == '[]'].index, inplace = True)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0gvYx0_2-fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1757a958-e6ba-492e-9213-b9e52e6df100"
      },
      "source": [
        "data_final.shape"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(399, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XBIn9GsyvTG"
      },
      "source": [
        "# Creating the final output file to use\n",
        "output_filename = 'News_Category.csv'\n",
        "output_filename = '/content/drive/My Drive/BigDataAnalyticsAndApplications/Project/dataset/'+ output_filename\n",
        "data_final.to_csv(output_filename, index= False)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDqvBWJ233su"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}